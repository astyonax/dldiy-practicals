{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first RNN\n",
    "\n",
    "The goal of this lesson is to introduce the ideas behind Recurrent Neural Networks.\n",
    "\n",
    "A very good starting point: [Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness)\n",
    "\n",
    "and the associated [code](https://github.com/karpathy/char-rnn) \n",
    "\n",
    "Fortunately, the code is not in pytorch, so that you can now 'translate it'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch_utils import ScaledEmbedding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_utils; \n",
    "from torch_utils import gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to download the collected works of Nietzsche to use as our data for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/nietzsche/'\n",
    "data_nie = data_folder+'nietzsche.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-12-30 13:40:45--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.72.226\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.72.226|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 600901 (587K) [text/plain]\n",
      "Saving to: ‘data/nietzsche/nietzsche.txt’\n",
      "\n",
      "data/nietzsche/niet 100%[===================>] 586.82K   546KB/s    in 1.1s    \n",
      "\n",
      "2017-12-30 13:40:46 (546 KB/s) - ‘data/nietzsche/nietzsche.txt’ saved [600901/600901]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p $data_folder\n",
    "!wget -O $data_nie 'https://s3.amazonaws.com/text-datasets/nietzsche.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 600901)\n"
     ]
    }
   ],
   "source": [
    "text = open(data_nie).read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total chars:', 86)\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars[1:-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map from chars to indices and back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idx will be the data we use from now own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is the'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to learn the 4-th character from the 3 first ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs=3\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.asarray(c1_dat[:-2])\n",
    "x2 = np.asarray(c2_dat[:-2])\n",
    "x3 = np.asarray(c3_dat[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.stack(c4_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([40, 30, 29,  1]), array([42, 25,  1, 43]), array([29, 27,  1, 45]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:4], x2[:4], x3[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 29,  1, 40])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200297,), (200297,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the architecture described in [Lesson 6 of fast.ai course](http://wiki.fast.ai/index.php/Lesson_6_Notes#Recurrent_Neural_Network_.28RNN.29:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim=42, vocab_size = 1, hidden_dim =256):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embeddings = ScaledEmbedding(vocab_size, embedding_dim)\n",
    "        self.i2h = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.h2h = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.h2o = nn.Linear(hidden_dim, vocab_size)\n",
    "            \n",
    "    def forward(self, c1, c2, c3):\n",
    "        c1_embedding = self.embeddings(c1)\n",
    "        c2_embedding = self.embeddings(c2)\n",
    "        c3_embedding = self.embeddings(c3)\n",
    "        c1_r = F.relu(self.i2h(c1_embedding))\n",
    "        c2_r = F.relu(self.i2h(c2_embedding))\n",
    "        c3_r = F.relu(self.i2h(c3_embedding))\n",
    "        h2 = F.tanh(self.h2h(c1_r))\n",
    "        h3 = F.tanh(self.h2h(h2+c2_r))\n",
    "        output = self.h2o(h3+c3_r)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = Variable(gpu(torch.from_numpy(np.array([idx[0]]).astype(np.int64))))\n",
    "in2 = Variable(gpu(torch.from_numpy(np.array([idx[1]]).astype(np.int64))))\n",
    "in3 = Variable(gpu(torch.from_numpy(np.array([idx[2]]).astype(np.int64))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn(in1,in2,in3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.0718  0.0170 -0.0124  0.0873 -0.0355  0.0238  0.0207 -0.1071  0.0914 -0.0193\n",
       "\n",
       "Columns 10 to 19 \n",
       "-0.0449 -0.0220  0.0314 -0.0239  0.0117  0.0291  0.0102 -0.1325 -0.0243  0.0210\n",
       "\n",
       "Columns 20 to 29 \n",
       "-0.0958 -0.0412 -0.0721  0.1280 -0.0649  0.0328  0.0071 -0.0180  0.0723 -0.0227\n",
       "\n",
       "Columns 30 to 39 \n",
       " 0.0166  0.0243  0.0028 -0.1259  0.0542 -0.0516  0.0035  0.0489  0.0038  0.0288\n",
       "\n",
       "Columns 40 to 49 \n",
       " 0.0087 -0.0094  0.0105 -0.0717 -0.0045  0.0248  0.0597  0.0036 -0.1202 -0.0477\n",
       "\n",
       "Columns 50 to 59 \n",
       " 0.0273  0.0537 -0.1072  0.0944 -0.0132 -0.0244 -0.0301  0.0042 -0.0240  0.0526\n",
       "\n",
       "Columns 60 to 69 \n",
       "-0.0413  0.0178 -0.0824 -0.0099 -0.0428  0.0343  0.0424 -0.0864  0.0085  0.0890\n",
       "\n",
       "Columns 70 to 79 \n",
       " 0.1179  0.0024  0.0078  0.0530 -0.0092  0.0247 -0.0111 -0.1049 -0.0862  0.1223\n",
       "\n",
       "Columns 80 to 85 \n",
       "-0.1020  0.0353 -0.0573 -0.0429  0.1549  0.0165\n",
       "[torch.FloatTensor of size 1x86]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with a batch of size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = Variable(gpu(torch.from_numpy(np.array([x1[:2]]).astype(np.int64))).squeeze())\n",
    "in2 = Variable(gpu(torch.from_numpy(np.array([x2[:2]]).astype(np.int64))).squeeze())\n",
    "in3 = Variable(gpu(torch.from_numpy(np.array([x3[:2]]).astype(np.int64))).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 40\n",
       " 30\n",
       "[torch.LongTensor of size 2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn(in1,in2,in3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.0718  0.0170 -0.0124  0.0873 -0.0355  0.0238  0.0207 -0.1071  0.0914 -0.0193\n",
       " 0.0677  0.0192 -0.0184  0.0853 -0.0506  0.0336  0.0075 -0.1016  0.0968 -0.0191\n",
       "\n",
       "Columns 10 to 19 \n",
       "-0.0449 -0.0220  0.0314 -0.0239  0.0117  0.0291  0.0102 -0.1325 -0.0243  0.0210\n",
       "-0.0444 -0.0152  0.0330 -0.0209  0.0175  0.0229  0.0075 -0.1290 -0.0200  0.0283\n",
       "\n",
       "Columns 20 to 29 \n",
       "-0.0958 -0.0412 -0.0721  0.1280 -0.0649  0.0328  0.0071 -0.0180  0.0723 -0.0227\n",
       "-0.0944 -0.0347 -0.0732  0.1261 -0.0691  0.0320  0.0136 -0.0378  0.0662 -0.0092\n",
       "\n",
       "Columns 30 to 39 \n",
       " 0.0166  0.0243  0.0028 -0.1259  0.0542 -0.0516  0.0035  0.0489  0.0038  0.0288\n",
       " 0.0079  0.0081 -0.0009 -0.1245  0.0452 -0.0495 -0.0010  0.0419  0.0054  0.0259\n",
       "\n",
       "Columns 40 to 49 \n",
       " 0.0087 -0.0094  0.0105 -0.0717 -0.0045  0.0248  0.0597  0.0036 -0.1202 -0.0477\n",
       " 0.0004 -0.0177  0.0177 -0.0703 -0.0094  0.0334  0.0638  0.0185 -0.1140 -0.0572\n",
       "\n",
       "Columns 50 to 59 \n",
       " 0.0273  0.0537 -0.1072  0.0944 -0.0132 -0.0244 -0.0301  0.0042 -0.0240  0.0526\n",
       " 0.0019  0.0605 -0.1081  0.0889 -0.0321 -0.0235 -0.0361 -0.0001 -0.0193  0.0589\n",
       "\n",
       "Columns 60 to 69 \n",
       "-0.0413  0.0178 -0.0824 -0.0099 -0.0428  0.0343  0.0424 -0.0864  0.0085  0.0890\n",
       "-0.0493  0.0218 -0.0795 -0.0155 -0.0348  0.0368  0.0533 -0.0735  0.0079  0.1051\n",
       "\n",
       "Columns 70 to 79 \n",
       " 0.1179  0.0024  0.0078  0.0530 -0.0092  0.0247 -0.0111 -0.1049 -0.0862  0.1223\n",
       " 0.1106  0.0103  0.0094  0.0603 -0.0001  0.0059 -0.0241 -0.1045 -0.0706  0.1536\n",
       "\n",
       "Columns 80 to 85 \n",
       "-0.1020  0.0353 -0.0573 -0.0429  0.1549  0.0165\n",
       "-0.0978  0.0270 -0.0709 -0.0382  0.1505  0.0315\n",
       "[torch.FloatTensor of size 2x86]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_loss = nn.CrossEntropyLoss()\n",
    "lr = 0.000001\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2Var(ch):\n",
    "    return Variable(gpu(torch.from_numpy(np.array([ch]).astype(np.int64))).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(c1,c2,c3,c4,batch_size=64,shuffle=True):\n",
    "    if shuffle:\n",
    "        index = np.random.permutation(c1.shape[0])\n",
    "        c1 = c1[index]\n",
    "        c2 = c2[index]\n",
    "        c3 = c3[index]\n",
    "        c4 = c4[index]\n",
    "    for idx in range(0,c1.shape[0],batch_size):\n",
    "        yield(c1[idx:idx+batch_size],c2[idx:idx+batch_size], c3[idx:idx+batch_size], c4[idx:idx+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(c1,c2,c3,c4,model=rnn,epochs=1,train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        batches = data_gen(c1,c2,c3,c4)\n",
    "        running_loss = 0.0\n",
    "        for ch1,ch2,ch3,ch4 in batches:\n",
    "            in1 = char2Var(ch1)#Variable(gpu(torch.from_numpy(np.array([ch1]).astype(np.int64))).squeeze())\n",
    "            in2 = char2Var(ch2)#Variable(gpu(torch.from_numpy(np.array([ch2]).astype(np.int64))).squeeze())\n",
    "            in3 = char2Var(ch3)#Variable(gpu(torch.from_numpy(np.array([ch3]).astype(np.int64))).squeeze())\n",
    "            ou4 = char2Var(ch4)#Variable(gpu(torch.from_numpy(np.array([ch4]).astype(np.int64))).squeeze())\n",
    "            \n",
    "            out = rnn(in1,in2,in3)\n",
    "            loss = rnn_loss(out,ou4)\n",
    "            rnn_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rnn_optimizer.step()\n",
    "            \n",
    "            running_loss += loss.data[0]\n",
    "            \n",
    "        epoch_loss = running_loss / c1.shape[0]\n",
    "        print('Loss: {:.4f}'.format(epoch_loss))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1113\n",
      "CPU times: user 47.8 ms, sys: 3.01 ms, total: 50.8 ms\n",
      "Wall time: 29.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(x1[:4], x2[:4], x3[:4], y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0685\n",
      "CPU times: user 1min 20s, sys: 2.03 s, total: 1min 22s\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(x1, x2, x3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0397\n",
      "CPU times: user 1min 16s, sys: 1.62 s, total: 1min 18s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(x1, x2, x3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [char2Var(i)[np.newaxis] for i in idxs]\n",
    "    out = rnn(arrs[0], arrs[1], arrs[2])\n",
    "    i = np.argmax(out.data.numpy())\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' an')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the Recurrent layers of pytorch. The documentation is [here](http://pytorch.org/docs/master/nn.html#rnn)\n",
    "\n",
    "To understand it, we will use a very simple example taken from PyTorchZeroToAll [tutorials](https://github.com/hunkim/PyTorchZeroToAll/blob/master/12_1_rnn_basics.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('inputs size', torch.Size([5, 4]))\n",
      "('one input size', torch.Size([1, 1, 4]), 'out size', torch.Size([1, 1, 2]), 'hidden size', torch.Size([1, 1, 2]))\n",
      "('one input size', torch.Size([1, 1, 4]), 'out size', torch.Size([1, 1, 2]), 'hidden size', torch.Size([1, 1, 2]))\n",
      "('one input size', torch.Size([1, 1, 4]), 'out size', torch.Size([1, 1, 2]), 'hidden size', torch.Size([1, 1, 2]))\n",
      "('one input size', torch.Size([1, 1, 4]), 'out size', torch.Size([1, 1, 2]), 'hidden size', torch.Size([1, 1, 2]))\n",
      "('one input size', torch.Size([1, 1, 4]), 'out size', torch.Size([1, 1, 2]), 'hidden size', torch.Size([1, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "\n",
    "# (batch, num_layers * num_directions, hidden_size) for batch_first=True\n",
    "hidden = (Variable(torch.randn(1, 1, 2)))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
    "print('inputs size', inputs.size())\n",
    "for one in inputs:\n",
    "    one = one.view(1, 1, -1)\n",
    "    # Input: (batch, seq_len, input_size) when batch_first=True\n",
    "    out, hidden = cell(one, hidden)\n",
    "    print(\"one input size\", one.size(), \"out size\", out.size(), 'hidden size', hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sequence input size', torch.Size([1, 5, 4]), 'out size', torch.Size([1, 5, 2]), 'hidden size', torch.Size([1, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "hidden = (Variable(torch.randn(1, 1, 2)))\n",
    "# We can do the whole at once\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = inputs.view(1, 5, -1)\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"sequence input size\", inputs.size(), \"out size\", out.size(), 'hidden size', hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('batch input size', torch.Size([3, 5, 4]), 'out size', torch.Size([3, 5, 2]), 'hidden size', torch.Size([1, 3, 2]))\n"
     ]
    }
   ],
   "source": [
    "# Strange batch number for hidden can be arbitrary?\n",
    "hidden = (Variable(torch.randn(3, 1, 2)))\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "# 3 batches 'hello', 'eolll', 'lleel'\n",
    "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
    "                                [e, o, l, l, l],\n",
    "                                [l, l, e, e, l]]))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "# B x S x I\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"batch input size\", inputs.size(), \"out size\", out.size(), 'hidden size', hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a bug: the hidden state has size (num_layers * num_directions, batch, hidden_size)?\n",
    "\n",
    "We are now ready to build our new RNN with an arbitrary number of characters in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class new_RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim=42, vocab_size = 1, hidden_dim =256):\n",
    "        super(new_RNN, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_dim = hidden_dim\n",
    "               \n",
    "        self.embeddings = ScaledEmbedding(vocab_size, embedding_dim)\n",
    "        self.cell = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.h2o = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = F.softmax\n",
    "        \n",
    "    def forward(self, c):\n",
    "        #print(c.size(1))\n",
    "        hidden = Variable(torch.zeros(c.size(0),1,self._hidden_dim))\n",
    "        for i in range(c.size(1)):\n",
    "            #print(self.embeddings(c[:,i]).size())\n",
    "            #print(self.embeddings(c[:,i]).unsqueeze(1).size(),hidden.size())\n",
    "            outp,hidden = self.cell(self.embeddings(c[:,i]).unsqueeze(1),hidden)\n",
    "            #print(i, outp.size())\n",
    "        output = self.h2o(outp)\n",
    "#         output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rnn = new_RNN(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = char2Var(idx[:8]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "   40    42    29    30    25    27    29     1\n",
       "[torch.LongTensor of size 1x8]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 40\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = ScaledEmbedding(vocab_size, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 30])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(inp[:,0]).unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = new_rnn(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "  -0.0037 -0.0466 -0.1427 -0.0551  0.0348  0.0384 -0.0661  0.0333  0.0169\n",
       "\n",
       "Columns 9 to 17 \n",
       "  -0.0306 -0.0655 -0.0020 -0.0534  0.0583 -0.0670  0.0251 -0.1005 -0.0304\n",
       "\n",
       "Columns 18 to 26 \n",
       "  -0.0505  0.0011 -0.0382 -0.0208 -0.0131  0.0861 -0.0114 -0.0634 -0.0473\n",
       "\n",
       "Columns 27 to 35 \n",
       "  -0.0225  0.0600 -0.0505 -0.0414 -0.0333 -0.0299  0.0343  0.0774  0.1006\n",
       "\n",
       "Columns 36 to 44 \n",
       "  -0.1005  0.0486 -0.0216 -0.0292 -0.0485  0.0431  0.0240  0.0827 -0.0338\n",
       "\n",
       "Columns 45 to 53 \n",
       "  -0.0799  0.0089  0.0333  0.0306 -0.0466 -0.0396  0.0446  0.0047  0.0131\n",
       "\n",
       "Columns 54 to 62 \n",
       "  -0.0511  0.0704  0.0324  0.0224 -0.0195 -0.0203  0.0074  0.0740  0.0607\n",
       "\n",
       "Columns 63 to 71 \n",
       "   0.0114 -0.0005  0.0579  0.0672 -0.0360 -0.0201  0.0246  0.0693 -0.0026\n",
       "\n",
       "Columns 72 to 80 \n",
       "   0.0118 -0.0309 -0.0331  0.0627 -0.0560  0.0022 -0.1064 -0.0071 -0.0088\n",
       "\n",
       "Columns 81 to 85 \n",
       "  -0.0442 -0.0814  0.0756  0.0332  0.0296\n",
       "[torch.FloatTensor of size 1x1x86]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 86])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+n] for i in range(0, len(idx)-1-cs, cs)]\n",
    "            for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 75112)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(c_in_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_out_dat = [idx[i+cs] for i in range(0, len(idx)-1-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [np.stack(c[:-2]) for c in c_in_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, (75110,))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xs), xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.stack(c_out_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75110,)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([40,  1, 33,  2, 72]),\n",
       " array([42,  1, 38, 44,  2]),\n",
       " array([29, 43, 31, 71, 54]),\n",
       " array([30, 45,  2, 74,  2]),\n",
       " array([25, 40, 73, 73, 76]),\n",
       " array([27, 40, 61, 61, 68]),\n",
       " array([29, 39, 54,  2, 66]),\n",
       " array([ 1, 43, 73, 62, 54])]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xs[n][:5] for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 33,  2, 72, 67])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1, 43, 45, 40, 40, 39, 43]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = char2Var([xs[n][:2] for n in range(cs)]).permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "   40    42    29    30    25    27    29     1\n",
       "    1     1    43    45    40    40    39    43\n",
       "[torch.LongTensor of size 2x8]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = new_rnn(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([40,  1, 33, ..., 72, 71, 61]),\n",
       " array([42,  1, 38, ..., 73, 65, 58]),\n",
       " array([29, 43, 31, ..., 62, 57,  2]),\n",
       " array([30, 45,  2, ..., 54,  2, 62]),\n",
       " array([25, 40, 73, ..., 67, 54, 67]),\n",
       " array([27, 40, 61, ...,  2, 72, 57]),\n",
       " array([29, 39, 54, ..., 76,  2, 62]),\n",
       " array([ 1, 43, 73, ..., 68, 73, 56])]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_x,_y = next(new_data_gen(np.asarray(xs),y))\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data_gen(ch,y,batch_size=64,shuffle=True):\n",
    "    if shuffle:\n",
    "        index = np.random.permutation(ch.shape[0])\n",
    "        ch = ch[index,:]\n",
    "        y = y[index]\n",
    "    for idx in range(0,ch.shape[0],batch_size):\n",
    "        yield(ch[idx:idx+batch_size,:], y[idx:idx+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_train_model(ch,y,model=new_rnn,epochs=10,train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        batches = new_data_gen(ch,y)\n",
    "        running_loss = 0.0\n",
    "        for ch,y in batches:\n",
    "            inp = char2Var(ch)\n",
    "            o = char2Var(y)\n",
    "            \n",
    "            out = new_rnn(inp)\n",
    "            #print(out.squeeze().size(), o.size())\n",
    "            loss = new_rnn_loss(out.squeeze(),o)\n",
    "            new_rnn_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            new_rnn_optimizer.step()\n",
    "            \n",
    "            running_loss += loss.data[0]\n",
    "            \n",
    "        epoch_loss = running_loss / ch.shape[0]\n",
    "        print('Loss: {:.4f}'.format(epoch_loss))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rnn_loss = nn.CrossEntropyLoss()\n",
    "lr = 0.000001\n",
    "new_rnn_optimizer = torch.optim.Adam(new_rnn.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3691\n",
      "Loss: 2.3683\n",
      "Loss: 2.3676\n",
      "Loss: 2.3668\n",
      "Loss: 2.3660\n",
      "Loss: 2.3652\n",
      "Loss: 2.3644\n",
      "Loss: 2.3637\n",
      "Loss: 2.3629\n",
      "Loss: 2.3621\n",
      "CPU times: user 443 ms, sys: 16 ms, total: 459 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_train_model(np.transpose([xs[n][:2] for n in range(cs)]),y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 86.0031\n",
      "Loss: 0.0826\n",
      "Loss: 0.0826\n",
      "Loss: 0.0826\n",
      "Loss: 0.0826\n",
      "Loss: 0.0826\n",
      "Loss: 0.0826\n",
      "Loss: 0.0826\n",
      "Loss: 0.0826\n",
      "Loss: 0.0826\n",
      "CPU times: user 1min 19s, sys: 1.82 s, total: 1min 21s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_train_model(np.transpose(xs),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "new_rnn_optimizer = torch.optim.Adam(new_rnn.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 86.9620\n",
      "Loss: 0.0651\n",
      "Loss: 0.0516\n",
      "Loss: 0.0391\n",
      "Loss: 0.0300\n",
      "Loss: 0.0236\n",
      "Loss: 0.0175\n",
      "Loss: 0.0130\n",
      "Loss: 0.0097\n",
      "Loss: 0.0070\n",
      "CPU times: user 1min 20s, sys: 1.96 s, total: 1min 22s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_train_model(np.transpose([xs[n] for n in range(cs)]),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_get_next(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    #print(idxs)\n",
    "    #arrs = [char2Var(i)[np.newaxis] for i in idxs]\n",
    "    arrs = char2Var(idxs).unsqueeze(0)\n",
    "    #print(arrs)\n",
    "    out = new_rnn(arrs)\n",
    "#     print out\n",
    "    i = np.argmax(out.data.numpy())\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_get_next('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_get_next('bues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise\n",
    "\n",
    "As stated during the course, this code is very preliminary and does not run on GPU. Fix it!\n",
    "\n",
    "Also instead of RNN, use [GRU](http://pytorch.org/docs/master/nn.html#torch.nn.GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
