<!DOCTYPE html>
<html>
  <head>
    <title>Deep Learning DIY lectures</title>
    <meta charset="utf-8">
    <style>
     .left-column {
       width: 50%;
       float: left;
     }
     .reset-column {
       overflow: auto;
        width: 100%;
     }
     .small { font-size: 0.2em; }
   
     .tiny { font-size: 12pt; }
     

     .right-column {
       width: 50%;
       float: right;
     }
     .footnote {
        position: absolute;
        bottom: 2em;
        margin: 1em 2em;
      }

      .credit {
        position: absolute;
        float:left;
        bottom: 0em;
        margin: 0em 0em;
        font-size: 0.4em;
      }

      .citation {
        /*float: left;*/
        bottom: 0em;
        /*margin: 2em 0em;*/
        margin: 0em 0em;
        /*position: absolute;*/
        color: #4B005F;
        font-style: italic;
        line-height: 100% !important;
      }

      .reset-column {
        overflow: auto;
         width: 100%;
      }

      .right{
        float:right;
      }

      .left{
        float:left;
      }
      .red { color: #ee1111; }
      .grey { color: #bbbbbb; }
      .green {color: #258212;}
      </style>
    <link rel="stylesheet" type="text/css" href="slides.css">
  </head>
  <body>
    <textarea id="source">
class: center, middle

# Lecture 5:
### Gradient descent, Backpropagation, Hand-crafted features, Neural Networks

Florent Krzakala - Marc Lelarge - Andrei Bursuc
<br/>
<br/>
.center[<img src="images/logos.png" style="width: 700px;" />]

.footnote.small[
With slides from A. Karpathy, F. Fleuret, J. Johnson, S. Yeung, A. Vedaldi ...]
<!-- .affiliations[
  ![ENS](images/logos.png)
]
 -->

---
## Recap

--

- Image classification:


- K-Nearest Neighbors: 


- Linear classifier: 


- Loss functions: Multi-class SVM and Softmax  


- Regularization  

---
## Recap
.center[<img src="images/enigma_1.png" style="width: 760px;" />]

---
## Recap
.center[<img src="images/enigma_2.png" style="width: 760px;" />]

---
## Recap
.center[<img src="images/enigma_3.png" style="width: 760px;" />]

---
## Today

.left[
- Gradient descent

- Backpropagation

- Hand crafted features

- FeedForward Networks

- Practical PyTorch: Clustering, Recsys, Triplet Loss
]

---

## Optimization

Given:
- a dataset of $(x,y)$
- a score function $s=f(x,W)=Wx$
- a loss function:
  + $L_i = -log\frac{e^{s_yi}}{\sum_j{e^{s_j}}}$ &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  .green[ per sample]
  + $L = \frac{1}{N}\sum^{N}_{i=1}{L_i} + R(W)$   .green[for all samples]

How to find best $W$?
Modularization in basic blocks helps building intuition (also for deep)

.center[<img src="images/comp_graph.png" height="150px"/>]


---
## Optimization

.center[
<img src="images/optimization_1.png" style="width: 760px;" />
]

---

## Optimization

.center[
<img src="images/optimization_2.png" style="width: 760px;" />
]

.center[Follow the slope!]

---

## Optimization

- Follow the slope
- In 1D, the derivative of a function: 

.center[$\frac{df(x)}{dx} = \lim_{h\to0}\frac{f(x+h)-f(x)}{h}$]

- In multiple dimensions, the gradient is a vector of partial 
derivatives along each dimension
  + The slope in any direction is the dot product of the (unit) direction with 
the gradient
  + The direction of the steepest descent is the negative gradient


---

## (Naive) finite differences

<img src="images/finite_diff_1.png" style="width: 760px;" /> 

.credit[Slide credit: J. Johnson]
---

## (Naive) finite differences

<img src="images/finite_diff_2.png" style="width: 760px;" /> 

.credit[Slide credit: J. Johnson]

---

## (Naive) finite differences

<img src="images/finite_diff_3.png" style="width: 760px;" /> 

.credit[Slide credit: J. Johnson]

---

## (Naive) finite differences

<img src="images/finite_diff_4.png" style="width: 760px;" /> 
.credit[Slide credit: J. Johnson]

---

## (Naive) finite differences

<img src="images/finite_diff_5.png" style="width: 760px;" /> 
.credit[Slide credit: J. Johnson]

---

## (Naive) finite differences

<img src="images/finite_diff_6.png" style="width: 760px;" /> 
.credit[Slide credit: J. Johnson]

---

## (Naive) finite differences

<img src="images/finite_diff_7.png" style="width: 760px;" /> 
.credit[Slide credit: J. Johnson]

---

## Optimization

- The loss function is just a function of $W$: 

.center[$L= \frac{1}{N}\sum^{N}_{i=1}{L_i} + \sum_k{W^2_k}$]

- We want $\nabla_WL$

- We can use calculus to compute an analytic gradient

- In practice: always use analytic gradient, but check implementation
with numerical gradient -> __gradient check__     

---

## Optimization

<img src="images/finite_diff_8.png" style="width: 760px;" /> 
.credit[Slide credit: J. Johnson]

---

## Gradient descent

- Code for simple gradient descent:

```python
# Vanilla Gradient Descent
while True:
  weights_grad = evaluate_gradient(loss_fun, data, weights)
  weights += - step_size * weights_grad # perform parameter update
```

.center[<img src="images/gradient_descent_1.png" height="300px"/>]
.credit[Slide credit: A. Karpathy]

---

## Gradient descent

- gradient descent uses local linear information to iteratively move towards a (local) minimum
- the iterative rule: &nbsp `weights += - step_size * weights_grad` corresponds to _"following the steepest descent"_
- this finds a local minimum and the choices of $w_0$ (initial weights) and `step_size` are important.

---
## Gradient descent
<br>
.center[<img src="images/gd_1.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_2.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_3.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_4.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_5.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_6.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_7.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_8.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_9.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_10.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_11.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_12.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]

---
## Gradient descent
<br>
.center[<img src="images/gd_13.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_14.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_15.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_16.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_17.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_18.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_19.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_20.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_21.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_22.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_23.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_24.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_25.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_26.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_27.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_28.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_29.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_30.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_31.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_32.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_33.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_34.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_35.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_36.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_grid_1.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_grid_2.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_grid_3.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---
## Gradient descent
<br>
.center[<img src="images/gd_grid_4.png" height="400px"/>]
.credit[Slide credit: F. Fleuret]
---

## Mini-batch gradient descent

- _a.k.a_ Stochastic Gradient Descent (SGD)
- Use only a small portion of the training set to compute the gradient

``` python
# Vanilla Gradient Descent
while True:
    data_batch = sample_training_data(data,128) # sample 128 examples
  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
  weights += - step_size * weights_grad # perform parameter update
```

- Common mini-batch sizes are 32/64/128/256 examples
- __step_size == learning rate__

.center[<img src="images/gradient_descent_2.png" height="150px"/>]

---

## Mini-batch gradient descent

- Example of optimization progress while training a neural network
- Showing loss over mini-batches as it goes down over time

.center[<img src="images/gradient_descent_3.png" height="400px"/>]

---

## Mini-batch gradient descent

- Example of optimization progress while training a neural network
- __Epoch__ = one full pass of the training dataset through the network


.center[<img src="images/gradient_descent_4.png" width="760px"/>]

.credit[Slide credit: A. Karpathy]

---
## Mini-batch gradient descent

- The effects of different optimization techniques .right.green.small[we'll cover them in more detail later on]

<br>
.center[<img src="images/gradient_descent_5.gif" height="350px"/>
]

---
## Backpropagation

Given:
- a dataset of $(x,y)$
- a score function $s=f(x,W)=Wx$
- a loss function:
  + $L_i = -log\frac{e^{s_yi}}{\sum_j{e^{s_j}}}$ .green[per sample]
  + $L = \frac{1}{N}\sum^{N}_{i=1}{L_i} + R(W)$ .green[for all samples]

How to find best $W$?
Modularization in basic blocks helps building intuition (also for deep)

.center[<img src="images/comp_graph_2.png" height="150px"/>]

.credit[Slide credit: A. Karpathy]

---

## Backpropagation

- Computational graphs
<br/>
<br/>
.center[<img src="images/comp_graph_3.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/alexnet.png" style="width: 780px"/>]
.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_1.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_2.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_3.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_4.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_5.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_1.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_6.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_7.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_8.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_9.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_10.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_11.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_12.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_unit_1.png" style="width: 780px"/>]

- What happens in a single unit/function/neuron/layer?

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_unit_2.png" style="width: 780px"/>]

- What happens in a single unit/function/neuron/layer?

.credit[Slide credit: A. Karpathy]

---
## Backpropagation
.center[<img src="images/backprop_unit_3.png" style="width: 780px"/>]
- What happens in a single unit/function/neuron/layer?

.credit[Slide credit: A. Karpathy]
---
## Backpropagation
.center[<img src="images/backprop_unit_4.png" style="width: 780px"/>]
- What happens in a single unit/function/neuron/layer?

.credit[Slide credit: A. Karpathy]
---
## Backpropagation
.center[<img src="images/backprop_unit_5.png" style="width: 780px"/>]
- What happens in a single unit/function/neuron/layer?

.credit[Slide credit: A. Karpathy]
---
## Backpropagation
.center[<img src="images/backprop_unit_6.png" style="width: 780px"/>]
- For _deep_ you just replicate modules in this manner

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_13.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_14.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_15.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_16.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_17.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_18.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_19.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_20.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_21.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_22.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_23.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_24.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_25.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---
## Backpropagation

- Another example: &nbsp $f(w,x) = \frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

.center[<img src="images/backprop_26.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---

## Backpropagation

- patterns in backward flow
  + __Add__ gate: distributes gradient evenly
  + __Max__ gate: gradient router to max input
  + __Mul__ gate: doing some sort of gradient switching between inputs

.center[<img src="images/backprop_patterns.png" style="height: 300px"/>]

.credit[Slide credit: A. Karpathy]
---

## Backpropagation

- gradients add at branches:

.center[<img src="images/backprop_branches.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---

## Backpropagation

- Implementation: forward/backward functions
  + (x,y,z) are scalars here

.center[<img src="images/backprop_api.png" style="width: 780px"/>]

.credit[Slide credit: A. Karpathy]
---

## Backpropagation - vectorized
- Example: $f(x,W)= \left\lVert W \cdot x \right\rVert^2 = \sum_{i=1}^{n}{(W \cdot x)^2_i}$ 
--

  + $ x \in \mathbb{R}^n$ 
  + $ W \in \mathbb{R}^{n \times n}$ 

--
.center[<img src="images/backprop_vect_1.png" style="height: 200px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_2.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_3.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_4.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_5.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_6.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_7.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_8.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_9.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_10.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_11.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_12.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_13.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_14.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

## Backpropagation - vectorized
.center[<img src="images/backprop_vect_15.png" style="width: 780px"/>]

.credit[Slide credit: S. Yeung]
---

class: center, middle
.center[<img src="images/leo_1.png" height="350px"/>]

---

class: center, middle
.center[<img src="images/leo_2.png" height="350px"/>]

---

class: center, middle

# Human-engineered features

### once upon a time ...

---

## Feature extraction - early methods

- Concatenation of pixels into 1D descriptors

.center[<img src="images/face_descriptors.png" style="height: 450px;"/>]

---

## Feature extraction - early methods

- Concatenation of pixels into 1D descriptors
- Applied to:
+ face recognition

.center[<img src="images/face_descriptors_2.png" style="height: 90px;"/>]

+ digit recognition

.center[<img src="images/digit_descriptors_1.png" style="height: 100px;"/>]

---

## Color histogram

- Histogram is a summary of the data describing in this case color characteristics

.center[<img src="images/color_histogram.png" style="height: 400px;"/>]

---

## Color histogram

- Histogram is a summary of the data describing in this case color characteristics

.center[<img src="images/color_histogram_2.png" style="height: 300px;"/>]

---

## Color histogram

Still used here and there 

.center[<img src="images/color_histogram_3.png" style="height: 400px;"/>]

---

## Color histogram

.center[<img src="images/color_histogram_4.png" style="width: 760px;"/>]

--
.center[<img src="images/color_histogram_5.png" style="width: 760px;"/>]

---

## Texture features

* Features corresponding to human perception
* Tamura examined 6 different features (found 3 to correspond strongly to human perception):
    - Coarseness -- coarse vs. fine
    - Contrast -- high vs. Low
    - Directionality -- directional vs. non-directional
    - Line-likeness -- line-like vs. non-line-like
    - Regularity -- regular vs. Irregular
    - Roughness -- rough vs. smooth

.center[<img src="images/textons.png" style="height: 100px;"/>]

---

## Gradient based representation

- Compute differences between sums of pixels in rectangles
- Captures contrast in adjacent spatial regions
- Similar to Haar wavelets, efficient to compute


.center[<img src="images/face_descriptors_3.png" style="width: 760px;"/>]

.center.citation.tiny[Rapid object detection using a boosted cascade of simple features, P. Viola, CVPR 2001]

---

## Histogram of Oriented Gradients (HOG)

.center[<img src="images/hog.png" style="width: 760px;"/>]

.center.citation.tiny[Histogram of oriented gradients for human detection, N. Dalal et al., CVPR 2005]
---

## Local features

- Identify small patterns of interest in the image (_i.e._ interest points, keypoints, corners)

.center[<img src="images/local_features_1.png" style="width: 760px;"/>]


---

## Local features - SIFT
- Describe content and context around interest points
- Scale Invariant Feature Transform (SIFT)
- Output is a $128d$ vector


.center[<img src="images/local_features_2.png" style="width: 760px;"/>]

.center.citation.tiny[Distinctive image features from scale-invariant keypoints, D. Lowe, IJCV 2004]

---

## Exhaustive matching

- Matching everything with everything

.center[<img src="images/local_features_3.png" style="width: 760px;"/>]

---

## Exhaustive matching

.center[<img src="images/local_features_4.png" style="height: 400px;"/>]

---
## Exhaustive matching

.center[<img src="images/local_features_5.png" style="height: 400px;"/>]

- The left image has $m$ features
- The right image has $n$ features

---
## Exhaustive matching

.center[<img src="images/local_features_6.png" style="height: 400px;"/>]

- Match the i-th left feature to its right nearest-neighbor nn(i), where
.center[<img src="images/nn_search_1.png" style="height: 40px;"/>]

---

## Exhaustive matching

.center[<img src="images/local_features_7.png" style="height: 400px;"/>]

---

## Exhaustive matching

.center[<img src="images/local_features_8.png" style="height: 400px;"/>]

---

## Going large scale?

--
<br>
.center[<img src="images/dog.png" style="height: 350px;"/>]

---

## Visual words

.center[<img src="images/visual_words_1.png" style="height: 400px;"/>]

.center.citation.tiny[Video Google: A text retrieval approach to object matching in videos, J. Sivic et al., ICCV 2003]
---

## Visual words

- Dictionary is typically learned using _k-means clustering_
- Value of $k$ depends on the task: from 8 to 16M

.center[<img src="images/voronoi.png" style="height: 350px;"/>]

---

## Visual words

- Visual word examples: each row is an equivalence class of patches mapped to the same cluster by _k-means_
- Visual words = iconic image fragments


.center[<img src="images/visual_words_2.png" style="height: 400px;"/>]

---

## Visual words

### Quantisation

.center[<img src="images/visual_words_3.png" style="height: 400px;"/>]

---

## Histogram of visual words

- A simple but efficient global image descriptor
- Vector of the number of occurrences of the $K$ visual words in the image (_i.e._ __embedding__)
- If there are $K$ visual words, then $h$ in $R^K$
- The vector $h$ is a global image descriptor
- $h$ is also called _bag of (visual) words (__BoW__)_

.center[<img src="images/visual_words_4.png" style="height: 300px;"/>]

---

## Histogram of visual words

### Intuition

.center[<img src="images/visual_words_5.png" style="height: 300px;"/>]

.center.citation.tiny[Video Google: A text retrieval approach to object matching in videos, J. Sivic et al., ICCV 2003]
---

## BoW extensions

### VLAD

- _Vector of Locally Aggregated Descriptors_

.center[<img src="images/vlad.png" style="height: 300px;"/>]

.center.citation.tiny[Aggregating local descriptors into a compact image representation, H. Jegou et al., CVPR 2010]

---

## BoW extensions

### Fisher Vectors


.center[<img src="images/fisher_vector.png" style="height: 300px;"/>]

.center.citation.tiny[Fisher kernels on visual vocabularies for image categorization, F. Perronning et al., ECCV 2010]

---

## BoW extensions

- dim(BoW) = $K$
     + $k$ = size of vocabulary
     + $k = [1e3, 1e4]$ for classification  
     + $k = [2e5, 16e6]$ for retrieval  

--

- dim(VLAD) = $K \times d$
     + $d$ = size of SIFT descriptors
     + $k = [64, 2048]$  

--

- dim(Fisher) = $K \times d \times 2$
     + $d$ = size of SIFT descriptors
     + $2$ = GMM moments
     + $k = [64, 2048]$  



---

## In the meantime ...

<br>
.center[<img src="images/fisher_vector_2.png" style="height: 300px;"/>]

---

class: center, middle
.center[<img src="images/leo_1.png" height="350px"/>]


---
## Why neural networks? Why _deep_?

- Traditional recognition: "shallow" architecture

  + each block is designed and implemented individually

.center[<img src="images/shallow_pipeline.png" style="width: 780px;"/>]

- Deep learning: "deep" architecture (Convolutional Neural Network)
.center[<img src="images/deep_pipeline.png" style="height: 200px;"/>]

---
## Why neural networks? Why _deep_?

- Deep learning: train and optimize all blocks jointly
  + 1 -- 140M trainable parameters

.center[<img src="images/deep_pipeline_2.png" style="width: 780px;"/>]

---
## Disclaimer

- Not trying to sell you the _Kool-Aid_ for doing only _end-to-end learning_
- _End-to-end_ worked quite well in the past few years
- Researchers have typically made differentiable classic operations from computer vision
- Domain experience is highly important and it's a key asset for progress in the next years  

.center[<img src="images/kool_aid.jpg" style="height: 250px;"/>]


---
class: center, middle

# Neural Networks


---
## Neural Network for classification

(__Before__) Linear score function: &nbsp $f = Wx$


---
## Neural Network for classification

(__Before__) Linear score function: &nbsp $f = Wx$


(__Now__) 2-layer neural network: &nbsp $f = W_2 max(0, W_1X)$


.footnote.center[<img src="images/basic_nn.png" style="height: 200px;"/>]

---
## Neural Network for classification

(__Before__) Linear score function: &nbsp $f = Wx$


(__Now__) 2-layer neural network: &nbsp $f = W_2 max(0, W_1X)$

Or a 3-layer neural network: &nbsp $f = W_3 max(0,W_2 max(0, W_1X))$

.footnote.center[<img src="images/basic_nn.png" style="height: 200px;"/>]

---
## Neural Network for classification

### The neuron
- Inspired by neuroscience and human brain, but resemblances do not go too far

.center[<img src="images/neuron.png" style="height: 200px;"/>]

- In fact there several types of neurons with different functions 
and the metaphor does not hold everywhere

.credit[Slide credit: A. Karpathy]

---
## Neural Network for classification

### The neuron
Inspired by neuroscience and human brain, but resemblances do not go too far

.center[<img src="images/neuron_2.png" style="height: 350px;"/>]
.credit[Slide credit: A. Karpathy]

---
## Neural Network for classification

Inspired by neuroscience and human brain, but resemblances do not go too far

.center[<img src="images/neuron_3.png" style="height: 350px;"/>]
.credit[Slide credit: A. Karpathy]

---
## Multi-layer neural networks

- __Training__: find network weights $w$ to minimize the error between 
true training labels $y_i$ and estimated labels $f_w(x_i)$: 

$$
E(w)= \sum_{i=1}^{N}{(y_i - f_w(x_i))^2}
$$

- Minimization can be done by gradient descent (if $f$ is differentiable)
  + the training method is called __backpropagation__
.center[<img src="images/mlp_2.png" style="height: 200px;"/>]


---
## Discovery of oriented cells in the visual cortex

.center[<img src="images/hubel.png" style="height: 400px;"/>]

.citation.center.tiny[Hubel& Wiesel, 1959]
---

## Discovery of oriented cells in the visual cortex

Find out more from [video](https://www.youtube.com/watch?v=IOHayh06LJ4)

.center[<img src="images/cat_neuron.png" style="height: 400px;"/>]

.citation.center.tiny[Hubel& Wiesel, 1959]


---

## Mark I Perceptron
- first implementation of the perceptron algorithm
- the machine was connected to a camera that used 20x20 cadmium sulfide photocells to produce a 400-pixel image
- it recognized letter of the alphabet

.left-column[
.center[<img src="images/percept_eq.png" style="height: 50px;"/>]
.center[<img src="images/percept_update.png" style="height: 50px;"/>]
]

.right-column[<img src="images/mark_1.png" style="height: 300px;"/>]

.reset-column[
]
.citation.center.tiny[Rosenblatt, 1957]


---
## Neural Network for classification

- Vector function with tunable parameters $\theta$ / $W$

$$
\mathbf{f}(\cdot; \mathbf{\theta}): \mathbb{R}^N \rightarrow (0, 1)^K
$$

- $s$ sample in dataset $S$:
  - input: $\mathbf{x}^s \in \mathbb{R}^N$
  - expected output: $y^s \in [0, K-1]$

- probability: $\mathbf{f}(\mathbf{x}^s;\mathbf{\theta})_c = p(Y=c|X=\mathbf{x}^s)$


.credit[Slide credit: C. Ollion & O. Grisel]

???
the model parametrizes a conditional distribution of Y given X

example:

- x is the vector of the pixel values of an photo in an online fashion
  store
- y is the type of the piece of closing (shoes, dress, shirt) represented
  in the photo
---

## Artificial Neuron

.center[
<img src="images/artificial_neuron.svg" style="width: 400px;" />
]

.credit[Slide credit: C. Ollion & O. Grisel]

--

<br/>
.center[
$z(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b$

$f(\mathbf{x}) = g(\mathbf{w}^T \mathbf{x} + b)$
]

- $\mathbf{x}, f(\mathbf{x}) \,\,$    input and output
- $z(\mathbf{x})\,\,$    pre-activation
- $\mathbf{w}, b\,\,$    weights and bias
- $g$ activation function

.credit[Slide credit: C. Ollion & O. Grisel]

???
McCullot & pitts: inspiration from brain, but simplistic model with no will to be close to biology
---

## More neurons -> more capacity

.center[
<img src="images/num_neurons.png" style="width: 780px;" />
]
---

## Layer of Neurons

.center[
<img src="images/neural_network.svg" style="width: 400px;" />
]

.credit[Slide credit: C. Ollion & O. Grisel]

--

<br/><br/>
.center[
$\mathbf{f}(\mathbf{x}) = g(\textbf{z(x)}) = g(\mathbf{W}  \mathbf{x} + \mathbf{b})$
]
<br/>
- $\mathbf{W}, \mathbf{b}\,\,$    now matrix and vector

.credit[Slide credit: C. Ollion & O. Grisel]
---
## One Hidden Layer Network

.center[
<img src="images/neural_network_hidden_1.svg" style="width: 700px;" />
]

<br/>
- $\mathbf{z}^h(\mathbf{x}) = \mathbf{W}^h \mathbf{x} + \mathbf{b}^h$
- <span style="color:#cccccc"> $\mathbf{h}(\mathbf{x}) = g(\mathbf{z}^h(\mathbf{x})) = g(\mathbf{W}^h \mathbf{x} + \mathbf{b}^h)$</span>
- <span style="color:#cccccc"> $\mathbf{z}^o(\mathbf{x}) = \mathbf{W}^o \mathbf{h}(\mathbf{x}) + \mathbf{b}^o$</span>
- <span style="color:#cccccc"> $\mathbf{f}(\mathbf{x}) = softmax(\mathbf{z}^o) = softmax(\mathbf{W}^o \mathbf{h}(\mathbf{x}) + \mathbf{b}^o)$</span>

.credit[Slide credit: C. Ollion & O. Grisel]

???
also named multi-layer perceptron (MLP)
feed forward, fully connected neural network
logistic regression is the same without the hidden layer

---
## One Hidden Layer Network

.center[
<img src="images/neural_network_hidden_2.svg" style="width: 700px;" />
]

<br/>
- <span style="color:#cccccc"> $\mathbf{z}^h(\mathbf{x}) = \mathbf{W}^h \mathbf{x} + \mathbf{b}^h$</span>
- $\mathbf{h}(\mathbf{x}) = g(\mathbf{z}^h(\mathbf{x})) = g(\mathbf{W}^h \mathbf{x} + \mathbf{b}^h)$
- <span style="color:#cccccc"> $\mathbf{z}^o(\mathbf{x}) = \mathbf{W}^o \mathbf{h}(\mathbf{x}) + \mathbf{b}^o$</span>
- <span style="color:#cccccc"> $\mathbf{f}(\mathbf{x}) = softmax(\mathbf{z}^o) = softmax(\mathbf{W}^o \mathbf{h}(\mathbf{x}) + \mathbf{b}^o)$</span>

.credit[Slide credit: C. Ollion & O. Grisel]

---
## One Hidden Layer Network

.center[
<img src="images/neural_network_hidden_3.svg" style="width: 700px;" />
]

<br/>
- <span style="color:#cccccc"> $\mathbf{z}^h(\mathbf{x}) = \mathbf{W}^h \mathbf{x} + \mathbf{b}^h$</span>
- <span style="color:#cccccc"> $\mathbf{h}(\mathbf{x}) = g(\mathbf{z}^h(\mathbf{x})) = g(\mathbf{W}^h \mathbf{x} + \mathbf{b}^h)$</span>
- $\mathbf{z}^o(\mathbf{x}) = \mathbf{W}^o \mathbf{h}(\mathbf{x}) + \mathbf{b}^o$
- <span style="color:#cccccc"> $\mathbf{f}(\mathbf{x}) = softmax(\mathbf{z}^o) = softmax(\mathbf{W}^o \mathbf{h}(\mathbf{x}) + \mathbf{b}^o)$</span>

.credit[Slide credit: C. Ollion & O. Grisel]

---
## One Hidden Layer Network

.center[
<img src="images/neural_network_hidden_4.svg" style="width: 700px;" />
]

<br/>
- <span style="color:#cccccc"> $\mathbf{z}^h(\mathbf{x}) = \mathbf{W}^h \mathbf{x} + \mathbf{b}^h$</span>
- <span style="color:#cccccc"> $\mathbf{h}(\mathbf{x}) = g(\mathbf{z}^h(\mathbf{x})) = g(\mathbf{W}^h \mathbf{x} + \mathbf{b}^h)$</span>
- <span style="color:#cccccc">$\mathbf{z}^o(\mathbf{x}) = \mathbf{W}^o \mathbf{h}(\mathbf{x}) + \mathbf{b}^o$</span>
- $\mathbf{f}(\mathbf{x}) = softmax(\mathbf{z}^o) = softmax(\mathbf{W}^o \mathbf{h}(\mathbf{x}) + \mathbf{b}^o)$

.credit[Slide credit: C. Ollion & O. Grisel]

---
## One Hidden Layer Network
.center[<img src="images/neural_network_hidden_t.svg" style="width: 700px;" />]

### Alternate representation
.center[<img src="images/flow_graph.svg" style="width: 500px;" /> ]

.credit[Slide credit: C. Ollion & O. Grisel]

---
## One Hidden Layer Network

.center[
<img src="images/neural_network_hidden_t.svg" style="width: 700px;" />
]
<br>
### PyTorch implementation
```py
model = torch.nn.Sequential(
    torch.nn.Linear(D_in, H),   # weight matrix dim [D_in x H]
    torch.nn.Tanh(),
    torch.nn.Linear(H, D_out),   # weight matrix dim [H x D_out]
    torch.nn.Softmax(),
)
```

---
## Element-wise activation functions
<br/>
.center[
<img src="images/activation_functions.svg" style="width: 780px;" />
]
<br/></br>
  - blue: activation function
  - green: derivative

.credit[Slide credit: C. Ollion & O. Grisel]

---
## Element-wise activation functions
- [Many other activation functions available](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/):
<br/>
.center[
<img src="images/activation_functions.png" style="height: 500px;" />
]

---
## Softmax function

$$
softmax(\mathbf{x}) = \frac{1}{\sum_{i=1}^{n}{e^{x_i}}}
\cdot
\begin{bmatrix}
  e^{x_1}\\\\
  e^{x_2}\\\\
  \vdots\\\\
  e^{x_n}
\end{bmatrix}
$$

$$
\frac{\partial softmax(\mathbf{x})_i}{\partial x_j} =
\begin{cases}
softmax(\mathbf{x})_i \cdot (1 - softmax(\mathbf{x})_i) & i = j\\\\
-softmax(\mathbf{x})_i \cdot softmax(\mathbf{x})_j & i \neq j
\end{cases}
$$


--

- vector of values in (0, 1) that add up to 1
- $p(Y = c|X = \mathbf{x}) = \text{softmax}(\mathbf{z}(\mathbf{(x}))_c$
- the pre-activation vector $\mathbf{z}(\mathbf{x})$ is often called "the logits"


.credit[Slide credit: C. Ollion & O. Grisel]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_1.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_2.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_3.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_4.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_5.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_6.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_7.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_8.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_9.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_10.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_11.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_12.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_13.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_14.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_15.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

---
## Universal approximation

We can approximate any $f \in \mathscr{C}([a,b],\mathbb{R})$ with a linear combination of translated/scaled ReLU functions

.center[
<img src="images/relu_16.png" style="height: 300px;" />
]

.credit[Slide credit: F. Fleuret]

This is true for other activation functions under mild assumptions
---
# Dropout

.center[
<img src="images/dropout.png" style="width: 680px;" />
]


.citation.tiny[Dropout: A Simple Way to Prevent Neural Networks from Overfitting,
Srivastava et al., JMLR 2014]

---
# Dropout

### Interpretation

- Reduces the network dependency to individual neurons
- More redundant representation of data

### Ensemble interpretation

- Equivalent to training a large ensemble of shared-parameters, binary-masked models
- Each model is only trained on a single data point

---
#Dropout

.center[
<img src="images/dropout_traintest.png" style="width: 600px;" /><br/>
]
<br/>

At test time, multiply weights by $p$ to keep same level of activation


.citation.tiny[Dropout: A Simple Way to Prevent Neural Networks from Overfitting,
Srivastava et al., JMLR 2014]


---

class: center, middle

# Applications of deep learning


---
## Deep Learning for Flow Sculpting

.center[
<img src="images/flow_sculpt_1.png" style="width: 600px;" /><br/>
]

.citation.tiny[Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data, D. Stoecklein et al., Nature Scientific Reports 2017]

---
## Deep Learning for Flow Sculpting

<br/>
.center[<img src="images/flow_sculpt_2.png" style="width: 600px;" /><br/> ]

---
## Deep Learning for Flow Sculpting

What about using a CNN?

.center[<img src="images/flow_sculpt_3.png" style="width: 600px;" /><br/> ]

---
## Deep Learning for Flow Sculpting

What about using a CNN?

.center[<img src="images/flow_sculpt_4.png" style="width: 600px;" /><br/> ]

---
## Deep Learning for Flow Sculpting

What about using a CNN?

.center[<img src="images/flow_sculpt_5.png" style="width: 600px;" /><br/> ]

---
## Recap


- Gradient descent

- Backpropagation

- Hand crafted features

- Intro to Neural Networks




</textarea>
    <style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
      });
      MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
         all[i].SourceElement().parentNode.className += ' has-jax';
         }
         });
         </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true
      });
    </script>
  </body>
</html>
